{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 7\n",
    "\n",
    "(прислать до 02:00 29.03.2020)\n",
    "\n",
    "Домашнее задание выполняется в этом же Jupyter Notebook'e и присылается мне в Piazza через Private Post.\n",
    "Файл с Вашим решением должен называться ```Surname_HW#```, где вместо ```Surname``` Вы пишите свою фамилию латиницей, а вместо ```#``` - номер домашнего задания. \n",
    "Решение каждой задачи необходимо поместить после её условия.\n",
    "\n",
    "Пожалуйста, пишите свои решения чётко и понятно. \n",
    "При полном запуске Вашего решения (Kernel -> Restart & Run All) все ячейки должны выполняться без ошибок. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Задача 1 (5 pts)\n",
    "\n",
    "Упражнение 2.1. из [книги лектора](https://arxiv.org/ftp/arxiv/papers/1711/1711.00394.pdf).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Решение\n",
    "\n",
    "Мы рассматриваем выпуклые негладкие функции, для которых $\\forall x, y \\in Q\\ \\ \\|\\nabla f(y) - \\nabla f(x) \\|_2 \\leq L_0 < \\infty$. \n",
    "\n",
    "Имеем оценку $f(\\overline x^N) - f(x^*) \\leq \\frac{LR^2}{2N}+\\delta\\ \\ \\ (2.22)$ для метода $x^{k+1} = \\pi_Q(x^k - h \\cdot \\nabla f(x^k)\\ \\ \\ (2.6)$ с длиной шага $h = \\frac{1}{L}$ (здесь $\\overline x^N = \\frac{1}{N}\\sum_{k=1}{N}x^k$).\n",
    "\n",
    "Ранее в книге было показано, что если $\\forall x, y\\ \\ \\|\\nabla f(y) - \\nabla f(x) \\|_2 \\leq L_{\\nu}\\|x-y\\|_2^{\\nu}$. Подставим   $\\nu=0$ и получим $L = \\frac{L_0^2}{2\\delta}$. Подставим выражение для $L$ в правую часть $(2.22)$ и получим $\\frac{L_0^2R^2}{4N\\delta} + \\delta$, ($R = \\|x^*-x^0\\|_2$). Минимизируем полученное выражение пo $\\delta$. Для этого продифференцируем это выражение пo $\\delta$ и приравняем производную к нулю:\n",
    "\n",
    "$$-\\frac{L_0^2R^2}{4N\\delta^2} + 1$\n",
    "\n",
    "$$\\delta^* = \\sqrt{\\frac{L_0^2R^2}{4N}} = \\frac{L_0R}{2\\sqrt{N}}$$\n",
    "\n",
    "Теперь подставим $\\delta = \\delta^*$ в правую часть оценки $(2.22)$. Получим\n",
    "\n",
    "$$f(\\overline x^N) - f(x^*) \\leq \\frac{L_0R}{\\sqrt{N}}$$\n",
    "\n",
    "Теперь помучаемся с нижней оценкой.\n",
    "\n",
    "По заданному $N \\leq n-1$ определим \n",
    "\n",
    "$$f(x) = F_{N+1}(x) = L_0 \\cdot \\max_{1 \\leq i \\leq N+1} x_i + \\frac{\\mu}{2} \\|x\\|^2_2 = L_0 \\cdot \\max_{1 \\leq i \\leq N+1} x_i + \\frac{\\mu}{2} \\sum_{i=1}^n x^2_i \\ \\ \\ (1)$$\n",
    "\n",
    "Здесь $\\mu = \\frac{L_0}{R\\sqrt{N+1}}$.\n",
    "\n",
    "Найдём $x^*$. Заметим, что в первом слагаемом целевой функции $\\max$ берётся только по первым $N+1$ компонентам вектора $x$. Остальные не влияют на этот $\\max$. То есть, компоненты $x_{N+2}, \\dots, x_n$ влияют только на сумму  $\\frac{\\mu}{2} \\sum_{i=1}^n x^2_i $. Видно, что минимум достигается при $x_{N+2} = \\dots = x_n=0$. Теперь рассмотрим первые $N+1$ компонент. Очевидно, что минимум достигается когда они не больше нуля. Тогда если $x_i < x_j \\leq 0$, то $x_i < x_j \\leq \\max\\limits_{1 \\leq k \\leq N+1} x_k$, а также $|x_i| > |x_j|$. А теперь мы можем изменить значение $x_i$ и увеличить его до $x_i = x_j$. При этом $\\max_{1 \\leq i \\leq N+1} x_i$ не изменится, а сумма квадратов уменьшится. Поэтому минимум достигается в точке, в которой $x_1 = \\dots = x_{N+1} = \\tau \\leq 0$.\n",
    "\n",
    "Итак, $x^*$ имеет вид $x^* = (\\tau, \\dots, \\tau, 0, \\dots, 0)$, где в начале $N+1$ буква $\\tau$. Подставим это в выражение $(1)$ и минимизируем его по $\\tau$. \n",
    "\n",
    "$$\\min_{\\tau}  L_0 \\tau + \\frac{\\mu(N+1)}{2} \\tau^2$$\n",
    "\n",
    "$$ L_0  + \\mu(N+1) \\tau^* = 0$$\n",
    "\n",
    "$$\\tau^* = -\\frac{L_0}{ \\mu(N+1)} = -\\frac{R}{\\sqrt{N+1}}$$\n",
    "\n",
    "$$f(x^*) = -\\frac{L_0R}{2\\sqrt{N+1}} < 0$$\n",
    "\n",
    "Пусть $x^0 = 0$.\n",
    "\n",
    "Докажем по индукции, что при $k \\leq N$ при $i > k$ выполнено $x_i^k=0$ с точностью до перестановки индексов. База понятна.\n",
    "\n",
    "Теперь предположим, что утверждение верно для шага $k-1$. \n",
    "\n",
    "Найдём решения уравнения $\\partial f(x) = 0$.\n",
    "\n",
    "$$ L_0 \\partial \\max_{1 \\leq i \\leq N+1} x_i + \\mu x = 0$$\n",
    "\n",
    "$$x = - \\frac{L_0}{\\mu} \\partial \\max_{1 \\leq i \\leq N+1}$$\n",
    "\n",
    "Субдифференциал $\\partial \\max_{1 \\leq i \\leq N+1} x^{k-1}_i = Lin\\{e_i | i \\in I\\}$, где $I$ -- множество индексов $i$, таких что $x^{k-1}_i = \\max_{1 \\leq i \\leq N+1} x^{k-1}_i = 0$. То есть, $\\partial \\max_{1 \\leq i \\leq N+1} x^{k-1}_i = Lin\\{e_k, \\dots, e_{N+1}\\}$. В худшем случае мы выберем из субифференциала какой-то орт $e_i$, так как нам надо оставить как можно больше нулевых компонент. Для порядка возьмём $e_k$. Тогда $x^k_{k+1} = \\dots = x^k_{N+1} = 0$, и $ \\max_{1 \\leq i \\leq N+1} x^k_i = 0$ в силу того что $k \\leq n$. Переход доказан.\n",
    "\n",
    "По доказанному выше $f(x^N) = L_0 \\max_{1 \\leq i \\leq N+1} x^N_i + \\frac{\\mu}{2} \\|x\\|^2_2 = 0 + \\frac{\\mu}{2} \\|x\\|^2_2  \\geq 0$. А значит, $f(x^N) - f(x^*) = F_{N+1}(x^N) - F_{N+1}(x^*) \\geq - F_{N+1}(x^*) = \\frac{L_0R}{2\\sqrt{N+1}} = \\frac{L_0^2}{2\\mu(N+1)}$. Оценка доказана."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 2 (5 pts)\n",
    "\n",
    "- (0.5 pts) Пусть Вам дано $m$ векторов из $\\mathbb{R}^n$, про которых известно что они сгенерированы из некоторого нормального распределения с параметрами $(\\mu, \\Sigma)$. Более того, известно, что $\\Sigma^{-1}$ разрежена, то есть много пар компонент векторов условно независимы. Сгенерируйте данные, которые удовлетворяют этим требованиям для $n = 100$ и $m = 500$. Если последующие пункты будут долго считаться, уменьшите эти параметры\n",
    "- (1 pts) Теперь поставьте задачу поиска оценки матрицы $\\Sigma^{-1}$ на основе оценки максимального правдоподобия\n",
    "- Добавьте к Вашей постановке ограничение на то, что матрица $\\Sigma^{-1}$ разрежена. Это делается добавлением к целевой функции слагаемого вида $\\lambda \\sum_{i < j} |(\\Sigma^{-1})_{ij}|$, то есть аналог $\\ell_1$ регуляризации для задачи наменьших квадратов, только теперь у нас разрежен не вектор, а матрица\n",
    "- (1 pts) Получите выражение для проксимального оператора для функции $\\lambda \\sum_{i < j} |(\\Sigma^{-1})_{ij}|$\n",
    "- (1 pts) Сравните сходимость субградиентного метода и проксимального градиентного метода для этой задачи\n",
    "- (1 pts) Проанализируйте как влияет значение параметра $\\lambda$ на близость решения к точному значению матрицы, которое использовалось при генерации данных\n",
    "- (0.5 pts) Сравните скорость решения рассмотренными методами со скоростью решения с помощью CVXPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.20209937 0.         0.         0.         0.        ]\n",
      " [0.         0.32157598 0.         0.         0.        ]\n",
      " [0.         0.         0.00200372 0.         0.        ]\n",
      " [0.         0.         0.         0.4962514  0.        ]\n",
      " [0.         0.         0.         0.         0.00921904]]\n",
      "[[  4.94806086   0.           0.           0.           0.        ]\n",
      " [  0.           3.10968499   0.           0.           0.        ]\n",
      " [  0.           0.         499.07238837   0.           0.        ]\n",
      " [  0.           0.           0.           2.01510766   0.        ]\n",
      " [  0.           0.           0.           0.         108.47113934]]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "m = 10\n",
    "\n",
    "mu = np.random.randn(n)\n",
    "sig = np.random.randn(n)\n",
    "sigma_inv05 = np.diag(sig) # Это матрица \\Sigma^{-1/2}\n",
    "print\n",
    "for i in range(n):\n",
    "    for j in range(i+1, n):\n",
    "        sigma_inv05[i][j] += np.random.binomial(size=1, n=1, p = 0.1) * np.random.randn(1)\n",
    "        sigma_inv05[j][i] = sigma_inv05[i][j]\n",
    "sigma_inv05 = sigma_inv05.dot(sigma_inv05.T) # Это матрица \\Sigma^{-1/2}\n",
    "sigma_inv = sigma_inv05.dot(sigma_inv05) # \\Sigma^{-1}\n",
    "sigma = np.linalg.inv(sigma_inv) # \\Sigma\n",
    "print(sigma_inv)\n",
    "print(sigma)\n",
    "\n",
    "V = np.random.randn(m, n).dot(np.linalg.inv(sigma_inv05))\n",
    "for i in range(m):\n",
    "    V[i] += mu\n",
    "print(len(V[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 1\n",
    "\n",
    "У нормальных случайных векторов есть своство: если $\\xi \\sim N(0, I)$, то $\\Sigma^{1/2}\\xi + \\mu \\sim N(\\mu, \\Sigma^{1/2}(\\Sigma^{1/2})^{\\top}) \\sim N(\\mu, \\Sigma)$. Это мы использовали в том числе и для генерации \n",
    "\n",
    "### Пункт 3\n",
    "\n",
    "Получим выражение для проксимального оператора функции $f(\\Sigma^{-1}) = \\lambda \\sum_{i < j} |(\\Sigma^{-1})_{ij}|$. Мы аппроксимируем искомую матрицу симметричной матрицей, поэтому можно считать, что проксимальный оператор определён только на симметричных матрицах.\n",
    "\n",
    "$$prox_{\\alpha f}(X) = \\arg\\min_U \\left( f(U) + \\frac{1}{2 \\alpha} \\|U-X\\|^2_F \\right) = \\arg\\min_U \\left( \\sum_{i < j}\\left( \\lambda |U_{ij}|+ \\frac{1}{ \\alpha} (U_{ij}-X_{ij})^2 \\right) \\right)$$\n",
    "\n",
    "В последней сумме слагаемые не зависят друг от друга, поэтому можно минимизировать по отдельности каждое из них.\n",
    "\n",
    "$$\\min_{U_{ij}}\\ \\  \\lambda |U_{ij}|+ \\frac{1}{ \\alpha} (U_{ij}-X_{ij})^2 $$\n",
    "\n",
    "Эту задачу решаем аналогично пункту 1 из задачи 3. Получаем, что \n",
    "\n",
    "\\begin{align}\n",
    "(prox_{\\alpha f}(X))_{ij} =\n",
    "\\begin{cases}\n",
    "x_{ij} - \\frac{\\lambda\\alpha}{2} & x_{ij} > \\frac{\\lambda\\alpha}{2}  \\\\\n",
    "0 &- \\frac{\\lambda\\alpha}{2}  \\leq x_{ij} \\leq \\frac{\\lambda\\alpha}{2} \\\\\n",
    "x_{ij} + \\frac{\\lambda\\alpha}{2}  & x_{ij} < -\\frac{\\lambda\\alpha}{2} \n",
    "\\end{cases}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача 3 (6 pts)\n",
    "\n",
    "- (2 pts) Выведите аналитический вид проксимального оператора для функции $|x|$\n",
    "\n",
    "- (2 pts) Покажите, что точка минимума $x^*$ выпуклой функции $f(x)$ является неподвижной точкой проксимального оператора, то есть\n",
    "\n",
    "$$\n",
    "x^* = prox_f(x^*)\n",
    "$$\n",
    "\n",
    "- (2 pts) Сравните сходимость (по итерациям и по времени) проксимального метода и градиентного спуска для задачи\n",
    "\n",
    " $$\n",
    " \\min \\frac{1}{2}x^\\top A x - b^{\\top}x,\n",
    " $$\n",
    " \n",
    " где $A \\in \\mathbb{S}^N_+$, при увеличении размера шага. Убедитесь, что градиентный спуск начинает расходиться, а проксимальный метод – нет. Как увеличение шага влияет на сходимость проксимального метода?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Пункт 1\n",
    "\n",
    "$$f(x) = |x|$$\n",
    "\n",
    "$$prox_{\\alpha f}(x) = \\arg\\min_u\\left( f(u) + \\frac{1}{2 \\alpha}\\|u-x\\|^2_2 \\right) = \\arg\\min_u\\left(|u| + \\frac{1}{2 \\alpha} (u-x)^2\\right)$$\n",
    "\n",
    "Найдём субдифференциал функции $g_x(u) = |u| + \\frac{1}{2 \\alpha} (u-x)^2$.\n",
    "\n",
    "\\begin{align}\n",
    "\\partial g_x(u) =\n",
    "\\begin{cases}\n",
    "1 + \\frac{1}{\\alpha}(u-x)  & u > 0 \\\\\n",
    "-1 + \\frac{1}{\\alpha}(u-x) & u < 0 \\\\\n",
    "[-1;1] + \\frac{1}{\\alpha}(u-x) & u=0\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "Решим уравнение $\\partial g_x(u) = 0$\n",
    "\n",
    "1. $$ 1 + \\frac{1}{\\alpha}(u-x) = 0,\\ u >0$$\n",
    "\n",
    "$$u = x - \\alpha$$\n",
    "\n",
    "Решение есть при $ x > \\alpha$.\n",
    "\n",
    "2. $$ -1 + \\frac{1}{\\alpha}(u-x) = 0,\\ u <0$$\n",
    "\n",
    "$$u = x + \\alpha$$\n",
    "\n",
    "Решение есть при $ x < -\\alpha$.\n",
    "\n",
    "3. $$k + \\frac{1}{\\alpha}(0-x) = 0,\\ k \\in [-1,1]$$\n",
    "\n",
    "$$k = \\frac{x}{\\alpha}$$\n",
    "\n",
    "Решение есть при $-\\alpha \\leq x \\leq \\alpha$.\n",
    "\n",
    "Функция $g_x(u)$ выпукла, поэтому решение уравнения $\\partial g_x(u) = 0$ будет минимумом функции.\n",
    "\n",
    "В итоге получаем\n",
    "\n",
    "\\begin{align}\n",
    "prox_{\\alpha f}(x) =\n",
    "\\begin{cases}\n",
    "x - \\alpha & x > \\alpha \\\\\n",
    "0 &- \\alpha \\leq x \\leq \\alpha\\\\\n",
    "x + \\alpha & x < -\\alpha\n",
    "\\end{cases}\n",
    "\\end{align}\n",
    "\n",
    "### Пункт 2\n",
    "\n",
    "Вычислим проксимацию в точке $x = x^*$.\n",
    "\n",
    "$$prox_{\\alpha f}(x^*) = \\arg\\min_u\\left( f(u) + \\frac{1}{2 \\alpha}\\|u-x^*\\|^2_2 \\right)$$\n",
    "\n",
    "$$\\forall u\\ \\ g(u) = f(u) + \\frac{1}{2 \\alpha}\\|u-x^*\\|^2_2 \\geq  f(x^*) + \\frac{1}{2 \\alpha}\\|x^*-x^*\\|^2_2 = f(x^*) = g(x^*)$$\n",
    "\n",
    "А значит, минимум функции $g(u) = f(u) + \\frac{1}{2 \\alpha}\\|u-x^*\\|^2_2$ достигается в точке $u = x^*$. Тогда \n",
    "\n",
    "$$prox_{\\alpha f}(x^*) = \\arg\\min_u g(u) = x^*$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.96268692 1.21801794 1.69464656 1.04608305 0.86611142]\n",
      " [1.21801794 1.64067514 1.03697439 0.96667309 0.89975298]\n",
      " [1.69464656 1.03697439 1.71225781 0.88036402 0.55180267]\n",
      " [1.04608305 0.96667309 0.88036402 0.67296078 0.59903033]\n",
      " [0.86611142 0.89975298 0.55180267 0.59903033 0.82139183]]\n",
      "[[3.96268692 1.21801794 1.69464656 1.04608305 0.86611142]\n",
      " [1.21801794 3.64067514 1.03697439 0.96667309 0.89975298]\n",
      " [1.69464656 1.03697439 3.71225781 0.88036402 0.55180267]\n",
      " [1.04608305 0.96667309 0.88036402 2.67296078 0.59903033]\n",
      " [0.86611142 0.89975298 0.55180267 0.59903033 2.82139183]]\n",
      "[[3.96268692 1.21801794 1.69464656 1.04608305 0.86611142]\n",
      " [1.21801794 3.64067514 1.03697439 0.96667309 0.89975298]\n",
      " [1.69464656 1.03697439 3.71225781 0.88036402 0.55180267]\n",
      " [1.04608305 0.96667309 0.88036402 2.67296078 0.59903033]\n",
      " [0.86611142 0.89975298 0.55180267 0.59903033 2.82139183]]\n",
      "[[ 1.00000000e+00  9.29327623e-19  7.87642115e-18 -5.47642981e-17\n",
      "   5.32045316e-17]\n",
      " [-7.91976538e-17  1.00000000e+00  5.66933081e-18  5.30983045e-17\n",
      "   4.88428808e-18]\n",
      " [-2.26042507e-16  6.03285429e-18  1.00000000e+00 -4.27558619e-17\n",
      "   2.23147041e-17]\n",
      " [-3.41620030e-17  2.26478705e-17 -4.25015602e-17  1.00000000e+00\n",
      "  -2.52626493e-17]\n",
      " [-2.98597365e-17 -3.11479661e-17 -2.25101510e-18 -2.75272583e-17\n",
      "   1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 5\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "A = np.random.rand(n, n)\n",
    "A = A.dot(A.T)\n",
    "b = np.random.randn(n)\n",
    "\n",
    "#A = np.mat(A)\n",
    "\n",
    "def descent(x0, eps, max_iter, alpha):\n",
    "    x = x0\n",
    "    for iter in range(max_iter):\n",
    "        grad = A * x - b\n",
    "        norm = np.linalg.norm(grad)\n",
    "        #print(np.linalg.norm(grad))\n",
    "        x = x - alpha * grad\n",
    "        if norm < eps:\n",
    "            break\n",
    "    return (x, iter + 1, norm)\n",
    "\n",
    "def prox(x0, eps, max_iter, alpha):\n",
    "    x = x0\n",
    "    for iter in range(max_iter):\n",
    "        grad = A.dot(x) - b\n",
    "        norm = np.linalg.norm(grad)\n",
    "        #print(np.linalg.norm(grad))\n",
    "        x = x - (np.linalg.inv(A + 1 / alpha * np.eye(n))) * grad\n",
    "        if norm < eps:\n",
    "            break\n",
    "    return (x, iter + 1, norm)\n",
    "\n",
    "alpha = 0.5\n",
    "print(A)\n",
    "print(A + 1 / alpha * np.eye(n))\n",
    "print(A + 1 / alpha * np.eye(n))\n",
    "print((A + 1 / alpha * np.eye(n)).dot( np.linalg.inv(A + 1 / alpha * np.eye(n))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Градиентный спуск\n",
      "10000 nan 10\n",
      "450 9.964584657407018e-08 1\n",
      "54 7.622718046032597e-08 0.5\n",
      "298 9.812141185007224e-08 0.1\n",
      "3045 9.958565870499373e-08 0.01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e664ade936a8>:19: RuntimeWarning: overflow encountered in multiply\n",
      "  x = x - alpha * grad\n",
      "<ipython-input-6-e664ade936a8>:19: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = x - alpha * grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 0.008353269242428452 0.001\n",
      "10000 2.0607735839467356 0.0001\n",
      "\n",
      "А теперь проксимальный метод\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e664ade936a8>:30: RuntimeWarning: overflow encountered in multiply\n",
      "  x = x - (np.linalg.inv(A + 1 / alpha * np.eye(n))) * grad\n",
      "<ipython-input-6-e664ade936a8>:30: RuntimeWarning: overflow encountered in subtract\n",
      "  x = x - (np.linalg.inv(A + 1 / alpha * np.eye(n))) * grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 nan 10\n",
      "10000 nan 1\n",
      "10000 1.8708922815312562e+65 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-e664ade936a8>:30: RuntimeWarning: invalid value encountered in subtract\n",
      "  x = x - (np.linalg.inv(A + 1 / alpha * np.eye(n))) * grad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 44.07241872851285 0.01\n",
      "10000 3.396917062785032e+20 0.05\n",
      "10000 6.632885443037115 0.001\n",
      "10000 11.566546543086112 0.0001\n"
     ]
    }
   ],
   "source": [
    "print(\"Градиентный спуск\")\n",
    "for h in [10,   1, 0.5, 0.1, 0.01,  1e-3, 1e-4]: \n",
    "    x, iter, norm = descent(np.ones(n), 1e-7, 10000, h)\n",
    "    print(iter, norm, h)\n",
    "print()\n",
    "print(\"А теперь проксимальный метод\")\n",
    "for h in [10, 1, 0.1, 0.01, 1e-3, 1e-4]: \n",
    "    x, iter, norm = prox(np.ones(n), 1e-7, 10000, h)\n",
    "    print(iter, norm, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -3.97250289  -1.18351654   0.53117777 ...  -1.95332019  65.02635074\n",
      "   19.25804156]\n",
      " [-47.34012896  -1.76801963   1.23687454 ...  -0.93090509   1.86330554\n",
      "    2.66424251]\n",
      " [ -2.31742736 -13.37279023   2.40563017 ...  -3.86952334   2.65361747\n",
      "   24.34492443]\n",
      " ...\n",
      " [ -1.90572477  -5.23330766   0.54331972 ...  -1.29407324   3.98546024\n",
      "    2.23332612]\n",
      " [ -1.8956478   -5.62567957   2.97165923 ...  -1.22845284   1.98925059\n",
      "   49.95143211]\n",
      " [-64.37986445  -3.0165273    2.61323607 ...  -0.88213534   4.15839744\n",
      "    9.56882066]] 999\n"
     ]
    }
   ],
   "source": [
    "print(x, iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получилось, что градиентный спуск сходится быстрее проксимального, да ещё и начинает расходиться при бОльших $h$. Видимо, проксимальный метод криво реализован..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
